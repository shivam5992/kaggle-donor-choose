{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## Shallow NN using Keras\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "from keras.layers import Input, Dense, Embedding\n",
    "from keras.layers import Flatten, concatenate, Dropout\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset preparation\n",
      "Label Encoding\n",
      "Robust Scaling\n"
     ]
    }
   ],
   "source": [
    "target_column = \"project_is_approved\"\n",
    "id_column = 'id'\n",
    "\n",
    "print \"dataset preparation\"\n",
    "normdf = pd.read_csv(\"../models/preprocessed/featured_2.csv\")\n",
    "testid = normdf[normdf['is_test'] == 1][id_column]\n",
    "\n",
    "Y = normdf[normdf['is_train'] == 1][target_column]\n",
    "drop_cols = [target_column, id_column]\n",
    "\n",
    "normdf = normdf.drop(drop_cols, axis = 1)\n",
    "\n",
    "print \"Label Encoding\"\n",
    "cat_feats =  ['project_grade_category', 'project_subject_categories', 'project_subject_subcategories', 'school_state', 'teacher_id', 'teacher_prefix']\n",
    "for c in cat_feats:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(normdf[c].astype(str))\n",
    "    normdf[c] = le.transform(normdf[c].astype(str))\n",
    "\n",
    "\n",
    "relevant_cols = \"\"\"teacher_id\n",
    "project_essay_2_density\n",
    "project_resource_summary_density\n",
    "resource_description_density\n",
    "project_essay_1_sub\n",
    "project_essay_2_sub\n",
    "project_essay_2_pol\n",
    "project_essay_1_density\n",
    "dayofyear_count\n",
    "project_essay_2_char_count\n",
    "project_essay_1_pol\n",
    "total_price\n",
    "price\n",
    "project_essay_1_char_count\n",
    "project_title_density\n",
    "project_essay_2_stopword\n",
    "project_essay_2_punctuation_count\n",
    "min_price\n",
    "min_total_price\n",
    "project_subject_subcategories_count\n",
    "resource_description_sub\n",
    "project_resource_summary_char_count\n",
    "resource_description_pol\n",
    "project_essay_2_word_count\n",
    "max_price\"\"\"\n",
    "relevant_cols = [x for x in relevant_cols.split(\"\\n\")]\n",
    "\n",
    "print \"Robust Scaling\"\n",
    "normdf = normdf.fillna(99)\n",
    "std = RobustScaler()\n",
    "normdf[relevant_cols] = pd.DataFrame(std.fit_transform(normdf[relevant_cols])).set_index(normdf.index)\n",
    "\n",
    "\n",
    "traindf = normdf[normdf['is_train'] == 1][relevant_cols]\n",
    "testdf = normdf[normdf['is_test'] == 1][relevant_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Vectors\n",
      "Stacking Vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "216"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print \"Loading Vectors\"\n",
    "textColumns = ['project_essay_1', 'project_essay_2', 'project_resource_summary', 'resource_description', 'project_title']\n",
    "tr_vects = []\n",
    "pred_vects_tr = []\n",
    "tr_vects_char = []\n",
    "\n",
    "for i, col in enumerate(textColumns):\n",
    "    tr_vect = sparse.load_npz(\"../models/vectors/new_tr_\"+col+\".npz\")\n",
    "    tr_vects.append(tr_vect)\n",
    "\n",
    "    tr_vect = sparse.load_npz(\"../models/vectors/new_char_tr_\"+col+\".npz\")\n",
    "    tr_vects_char.append(tr_vect)\n",
    "\n",
    "print \"Stacking Vectors\"\n",
    "num_train = normdf[normdf['is_train'] == 1][relevant_cols]\n",
    "tr_vect1 = hstack(tr_vects, 'csr')\n",
    "tr_vect2 = hstack(tr_vects_char, 'csr')\n",
    "del tr_vects, tr_vects_char\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pseudo Labelling \n",
    "pseudo = False\n",
    "if pseudo:\n",
    "    pred_vects_tr.append(csr_matrix(traindf))\n",
    "    X_train_stack = hstack(pred_vects_tr, 'csr')\n",
    "\n",
    "    train_preds = []\n",
    "    predsdf = pd.DataFrame()\n",
    "    for i in range(1,10):\n",
    "        print i \n",
    "        model = lgb.Booster(model_file='../models/uplgb'+str(i)+'.txt')\n",
    "        predsdf[\"weak_\"+str(i)] = model.predict(X_train_stack, num_iteration=model.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf1, X_valid_tfidf1, y_train, y_valid = train_test_split(tr_vect1, Y, test_size=0.20, random_state=42)\n",
    "X_train_tfidf2, X_valid_tfidf2, y_train, y_valid = train_test_split(tr_vect2, Y, test_size=0.20, random_state=42)\n",
    "X_train1, X_valid1, y_train, y_valid = train_test_split(num_train, Y, test_size=0.20, random_state=42)\n",
    "\n",
    "\n",
    "X_train_target = y_train\n",
    "size_tfidf1 = X_train_tfidf1.shape[1]\n",
    "size_tfidf2 = X_train_tfidf2.shape[1]\n",
    "size_numfeats1 = len(X_train1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Input Layer of our model\n",
    "input_num1 = Input((size_numfeats1, ))\n",
    "input_tfidf1 = Input((size_tfidf1, ), sparse=True)\n",
    "input_tfidf2 = Input((size_tfidf2, ), sparse=True)\n",
    "\n",
    "layer_num1 = Dense(512, activation='relu')(input_num1)\n",
    "\n",
    "layer_tfidf1 = Dense(256, activation=\"relu\")(input_tfidf1)\n",
    "layer_tfidf1 = Dense(512, activation=\"relu\")(input_tfidf1)\n",
    "\n",
    "layer_tfidf2 = Dense(256, activation=\"relu\")(input_tfidf2)\n",
    "\n",
    "output = concatenate([layer_num1, layer_tfidf1, layer_tfidf2])\n",
    "output = Dense(512, activation=\"relu\")(output)\n",
    "\n",
    "output = Dense(1, activation=\"sigmoid\")(output)\n",
    "\n",
    "# combine the model\n",
    "model = Model(inputs=[input_num1, input_tfidf1, input_tfidf2], outputs=output)\n",
    "model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 116531 samples, validate on 29133 samples\n",
      "Epoch 1/1\n",
      "116531/116531 [==============================] - 105s 901us/step - loss: 0.3537 - acc: 0.8570 - val_loss: 0.3560 - val_acc: 0.8568\n",
      "0.7833896895200388\n"
     ]
    }
   ],
   "source": [
    "# Train the model \n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "model.fit([X_train1, X_train_tfidf1, X_train_tfidf2], X_train_target, validation_split=0.20, batch_size=512, epochs=1, callbacks=[earlystop])\n",
    "\n",
    "val_pred = model.predict([X_valid1, X_valid_tfidf1, X_valid_tfidf2])\n",
    "print roc_auc_score(y_valid, val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predicting on test data\n",
    "\n",
    "print \"Loading Vectors\"\n",
    "textColumns = ['project_essay_1', 'project_essay_2', 'project_resource_summary', 'resource_description', 'project_title']\n",
    "ts_vects = []\n",
    "ts_vects_char = []\n",
    "for i, col in enumerate(textColumns):\n",
    "    ts_vect = sparse.load_npz(\"models/vectors/new_\"+col+\".npz\")\n",
    "    ts_vects.append(ts_vect)\n",
    "    \n",
    "    ts_vect = sparse.load_npz(\"models/vectors/new_char_\"+col+\".npz\")\n",
    "    ts_vects_char.append(ts_vect)\n",
    "\n",
    "print \"stacking\"\n",
    "ts_vect1 = hstack(ts_vects, 'csr')\n",
    "ts_vect2 = hstack(ts_vects_char, 'csr')\n",
    "num_test = normdf[normdf['is_test'] == 1][relevant_cols]\n",
    "\n",
    "print \"predicting\"\n",
    "test_preds = model.predict([num_test, ts_vect1, ts_vect2])\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['id'] = testid\n",
    "sub['project_is_approved'] = test_preds\n",
    "# delete Models \n",
    "\n",
    "sub.to_csv(\"sub/nn_ps.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "866"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc \n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final AUC Scores: \n",
    "# 0.7791028157116862 - 512,256,512 (no dropout)\n",
    "# 0.7811179176857501 - same + chars (train los : 0.3711)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
