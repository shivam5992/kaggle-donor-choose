{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## NN\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "from keras.layers import Input, Dense, Embedding\n",
    "from keras.layers import Flatten, concatenate, Dropout\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset preparation\n",
      "Label Encoding\n",
      "Robust Scaling\n"
     ]
    }
   ],
   "source": [
    "target_column = \"project_is_approved\"\n",
    "id_column = 'id'\n",
    "\n",
    "print \"dataset preparation\"\n",
    "normdf = pd.read_csv(\"../models/preprocessed/featured_2.csv\")\n",
    "testid = normdf[normdf['is_test'] == 1][id_column]\n",
    "\n",
    "Y = normdf[normdf['is_train'] == 1][target_column]\n",
    "drop_cols = [target_column, id_column]\n",
    "\n",
    "normdf = normdf.drop(drop_cols, axis = 1)\n",
    "\n",
    "print \"Label Encoding\"\n",
    "cat_feats =  ['project_grade_category', 'project_subject_categories', 'project_subject_subcategories', 'school_state', 'teacher_id', 'teacher_prefix']\n",
    "for c in cat_feats:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(normdf[c].astype(str))\n",
    "    normdf[c] = le.transform(normdf[c].astype(str))\n",
    "\n",
    "\n",
    "relevant_cols = \"\"\"teacher_id\n",
    "project_essay_2_density\n",
    "project_resource_summary_density\n",
    "resource_description_density\n",
    "project_essay_1_sub\n",
    "project_essay_2_sub\n",
    "project_essay_2_pol\n",
    "project_essay_1_density\n",
    "dayofyear_count\n",
    "project_essay_2_char_count\n",
    "project_essay_1_pol\n",
    "total_price\n",
    "price\n",
    "project_essay_1_char_count\n",
    "project_title_density\n",
    "project_essay_2_stopword\n",
    "project_essay_2_punctuation_count\n",
    "min_price\n",
    "min_total_price\n",
    "project_subject_subcategories_count\n",
    "resource_description_sub\n",
    "project_resource_summary_char_count\n",
    "resource_description_pol\n",
    "project_essay_2_word_count\n",
    "max_price\"\"\"\n",
    "relevant_cols = [x for x in relevant_cols.split(\"\\n\")]\n",
    "\n",
    "print \"Robust Scaling\"\n",
    "normdf = normdf.fillna(99)\n",
    "std = RobustScaler()\n",
    "normdf[relevant_cols] = pd.DataFrame(std.fit_transform(normdf[relevant_cols])).set_index(normdf.index)\n",
    "\n",
    "\n",
    "traindf = normdf[normdf['is_train'] == 1][relevant_cols]\n",
    "testdf = normdf[normdf['is_test'] == 1][relevant_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Vectors\n",
      "Stacking Vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "220"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print \"Loading Vectors\"\n",
    "textColumns = ['project_essay_1', 'project_essay_2', 'project_resource_summary', 'resource_description', 'project_title']\n",
    "tr_vects = []\n",
    "pred_vects_tr = []\n",
    "\n",
    "# tr_vects_char = [] ## new\n",
    "\n",
    "for i, col in enumerate(textColumns):\n",
    "    tr_vect = sparse.load_npz(\"../models/vectors/new_tr_\"+col+\".npz\")\n",
    "    tr_vects.append(tr_vect)\n",
    "    \n",
    "    \n",
    "#     pred_vects_tr.append(tr_vect) ## new\n",
    "\n",
    "    tr_vect = sparse.load_npz(\"../models/vectors/new_char_tr_\"+col+\".npz\")\n",
    "    tr_vects_char.append(tr_vect)\n",
    "\n",
    "print \"Stacking Vectors\"\n",
    "num_train = normdf[normdf['is_train'] == 1][relevant_cols]\n",
    "tr_vect1 = hstack(tr_vects, 'csr')\n",
    "tr_vect2 = hstack(tr_vects_char, 'csr')\n",
    "del tr_vects, tr_vects_char\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## new \n",
    "# pred_vects_tr.append(csr_matrix(traindf))\n",
    "# X_train_stack = hstack(pred_vects_tr, 'csr')\n",
    "\n",
    "# train_preds = []\n",
    "# predsdf = pd.DataFrame()\n",
    "# for i in range(1,10):\n",
    "#     print i \n",
    "#     model = lgb.Booster(model_file='../models/uplgb'+str(i)+'.txt')\n",
    "#     predsdf[\"weak_\"+str(i)] = model.predict(X_train_stack, num_iteration=model.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf1, X_valid_tfidf1, y_train, y_valid = train_test_split(tr_vect1, Y, test_size=0.20, random_state=42)\n",
    "X_train_tfidf2, X_valid_tfidf2, y_train, y_valid = train_test_split(tr_vect2, Y, test_size=0.20, random_state=42)\n",
    "X_train1, X_valid1, y_train, y_valid = train_test_split(num_train, Y, test_size=0.20, random_state=42)\n",
    "X_train2, X_valid2, y_train, y_valid = train_test_split(predsdf, Y, test_size=0.20, random_state=42)\n",
    "\n",
    "\n",
    "X_train_target = y_train\n",
    "size_tfidf1 = X_train_tfidf1.shape[1]\n",
    "size_tfidf2 = X_train_tfidf2.shape[1]\n",
    "size_numfeats1 = len(X_train1.columns)\n",
    "size_numfeats2 = len(predsdf.columns)\n",
    "\n",
    "# Complete Training Purposes\n",
    "\n",
    "# X_train_tfidf1 = tr_vect1\n",
    "# X_train_tfidf2 = tr_vect2\n",
    "# X_train1 = num_train\n",
    "# X_train_target = Y\n",
    "\n",
    "# size_tfidf1 = X_train_tfidf1.shape[1]\n",
    "# size_tfidf2 = X_train_tfidf2.shape[1]\n",
    "# size_numfeats1 = len(X_train1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Input Layer of our model\n",
    "input_num1 = Input((size_numfeats1, ))\n",
    "input_tfidf1 = Input((size_tfidf1, ), sparse=True)\n",
    "input_tfidf2 = Input((size_tfidf2, ), sparse=True)\n",
    "\n",
    "layer_num1 = Dense(512, activation='relu')(input_num1)\n",
    "\n",
    "layer_tfidf1 = Dense(256, activation=\"relu\")(input_tfidf1)\n",
    "layer_tfidf1 = Dense(512, activation=\"relu\")(input_tfidf1)\n",
    "\n",
    "layer_tfidf2 = Dense(256, activation=\"relu\")(input_tfidf2)\n",
    "\n",
    "output = concatenate([layer_num1, layer_tfidf1, layer_tfidf2])\n",
    "output = Dense(512, activation=\"relu\")(output)\n",
    "\n",
    "output = Dense(1, activation=\"sigmoid\")(output)\n",
    "\n",
    "# combine the model\n",
    "model = Model(inputs=[input_num1, input_tfidf1, input_tfidf2], outputs=output)\n",
    "model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 116531 samples, validate on 29133 samples\n",
      "Epoch 1/5\n",
      "116531/116531 [==============================] - 110s 941us/step - loss: 0.3763 - acc: 0.8504 - val_loss: 0.3569 - val_acc: 0.8562\n",
      "Epoch 2/5\n",
      "116531/116531 [==============================] - 105s 904us/step - loss: 0.3286 - acc: 0.8670 - val_loss: 0.3608 - val_acc: 0.8552\n",
      "Epoch 3/5\n",
      " 60928/116531 [==============>...............] - ETA: 45s - loss: 0.2591 - acc: 0.8991"
     ]
    }
   ],
   "source": [
    "# Train the model \n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "model.fit([X_train1, X_train_tfidf1, X_train_tfidf2], X_train_target, validation_split=0.20, batch_size=512, epochs=5, callbacks=[earlystop])\n",
    "\n",
    "val_pred = model.predict([X_valid1, X_valid_tfidf1, X_valid_tfidf2])\n",
    "print roc_auc_score(y_valid, val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Vectors\n",
      "stacking\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-aac5e6e69a58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"stacking\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mts_vect1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts_vects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mts_vect2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts_vects_char\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mnum_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnormdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_test'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrelevant_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/scipy/sparse/construct.pyc\u001b[0m in \u001b[0;36mhstack\u001b[0;34m(blocks, format, dtype)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m     \"\"\"\n\u001b[0;32m--> 464\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/scipy/sparse/construct.pyc\u001b[0m in \u001b[0;36mbmat\u001b[0;34m(blocks, format, dtype)\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrow_offsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_offsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnnz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m     \u001b[0midx_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_index_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m     \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnnz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## predicting on test data\n",
    "\n",
    "print \"Loading Vectors\"\n",
    "textColumns = ['project_essay_1', 'project_essay_2', 'project_resource_summary', 'resource_description', 'project_title']\n",
    "ts_vects = []\n",
    "ts_vects_char = []\n",
    "for i, col in enumerate(textColumns):\n",
    "    ts_vect = sparse.load_npz(\"models/vectors/new_\"+col+\".npz\")\n",
    "    ts_vects.append(ts_vect)\n",
    "    \n",
    "    ts_vect = sparse.load_npz(\"models/vectors/new_char_\"+col+\".npz\")\n",
    "    ts_vects_char.append(ts_vect)\n",
    "\n",
    "print \"stacking\"\n",
    "ts_vect1 = hstack(ts_vects, 'csr')\n",
    "ts_vect2 = hstack(ts_vects_char, 'csr')\n",
    "num_test = normdf[normdf['is_test'] == 1][relevant_cols]\n",
    "\n",
    "print \"predicting\"\n",
    "test_preds = model.predict([num_test, ts_vect1, ts_vect2])\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['id'] = testid\n",
    "sub['project_is_approved'] = test_preds\n",
    "sub.to_csv(\"sub/nn_ps.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "866"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc \n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.7791028157116862 - 512,256,512 (no dropout)\n",
    "# 0.7811179176857501 - same + chars (train los : 0.3711)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
