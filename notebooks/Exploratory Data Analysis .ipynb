{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import plotly.offline as py\n",
    "from plotly import tools\n",
    "import seaborn as sns\n",
    "\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import string\n",
    "\n",
    "color = sns.color_palette()\n",
    "warnings.filterwarnings('ignore')\n",
    "py.init_notebook_mode(connected=True)\n",
    "%matplotlib inline\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = 999\n",
    "punctuation = string.punctuation\n",
    "\n",
    "stop_words = open('data/stopwords.txt').read().strip().split(\"\\n\")\n",
    "stop_words = [x.replace(\"\\r\",\"\") for x in stop_words]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "id_column = \"id\"\n",
    "missing_token = \" UNK \"\n",
    "\n",
    "train = pd.read_csv(\"data/train.csv\", index_col=id_column, parse_dates=[\"project_submitted_datetime\"])\n",
    "test = pd.read_csv(\"data/test.csv\", index_col=id_column, parse_dates=[\"project_submitted_datetime\"])\n",
    "\n",
    "df = pd.concat([train, test], axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "rc = pd.read_csv(\"data/resources.csv\", index_col=id_column).fillna(missing_token)\n",
    "rc['total_price'] = rc.quantity * rc.price\n",
    "rc['price_sum'] = rc['price'].copy()\n",
    "rc['quantity_sum'] = rc['quantity'].copy()\n",
    "rc['quantity_count'] = rc['quantity'].copy()\n",
    "\n",
    "rc = rc.reset_index().groupby(id_column).agg(dict(quantity_count='count', price_sum='sum', quantity_sum='sum', \n",
    "                                                  total_price='mean', quantity='mean', price='mean', \n",
    "                                                  description=lambda x: missing_token.join(x)))\n",
    "df = pd.merge(df, rc, left_index=True, right_index=True, how= \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_text(x):\n",
    "    x = x.replace(\"\\\\r\", \" \").replace(\"\\\\t\", \" \").replace(\"\\\\n\", \" \")\n",
    "    x = \"\".join(_ for _ in x if _ not in punctuation)\n",
    "    x = x.lower()\n",
    "    return x\n",
    "\n",
    "def get_polarity(text):\n",
    "    try:\n",
    "        textblob = TextBlob(text)\n",
    "        pol = textblob.sentiment.polarity\n",
    "    except Exception as E:\n",
    "        pol = 0.0\n",
    "    return pol\n",
    "\n",
    "def get_subjectivity(text):\n",
    "    try:\n",
    "        textblob = TextBlob(text)\n",
    "        subj = textblob.sentiment.subjectivity\n",
    "    except:\n",
    "        subj = 0.0\n",
    "    return subj\n",
    "\n",
    "pos_dic = {\n",
    "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
    "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
    "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
    "    'adj' :  ['JJ','JJR','JJS'],\n",
    "    'adv' : ['RB','RBR','RBS','WRB']\n",
    "}\n",
    "\n",
    "def pos_check(x, flag):\n",
    "    cnt = 0\n",
    "    try:\n",
    "        wiki = TextBlob(x)\n",
    "        for tupe in wiki.tags:\n",
    "            ppo = list(tupe)[1]\n",
    "            if ppo in pos_dic[flag]:\n",
    "                cnt += 1\n",
    "    except Exception as E:\n",
    "        pass\n",
    "    return cnt \n",
    "\n",
    "def add_count_cat(tx, mapping):\n",
    "    return sum([mapping[_.strip()] for _ in tx.split(\",\")]) / len(tx.split(\",\"))\n",
    "\n",
    "def getCountVar(compute_df, var_name, splitter=False):\n",
    "    if splitter:\n",
    "        values = []\n",
    "        for each in df[var_name]:\n",
    "            allval = each.split(\",\")\n",
    "            allval = [x.strip() for x in allval]\n",
    "            values.extend(allval)\n",
    "        value_counts = dict(Counter(values))  \n",
    "        compute_df[\"Count_\"+var_name] = compute_df[var_name].apply(lambda x: add_count_cat(x, value_counts))    \n",
    "    else:\n",
    "        grouped_df = compute_df.groupby(var_name, as_index=False).agg('size').reset_index()\n",
    "        grouped_df.columns = [var_name, \"var_count\"]\n",
    "        merged_df = pd.merge(compute_df, grouped_df, how=\"left\", on=var_name)\n",
    "        merged_df.fillna(-1, inplace=True)\n",
    "        compute_df[\"Count_\"+var_name] = list(merged_df[\"var_count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['project_essay_3'] = df['project_essay_3'].fillna(missing_token)\n",
    "df['project_essay_4'] = df['project_essay_4'].fillna(missing_token)\n",
    "\n",
    "df[\"essay1_len\"] = df['project_essay_1'].apply(len)\n",
    "df[\"essay2_len\"] = df['project_essay_2'].apply(len)\n",
    "df[\"essay3_len\"] = df['project_essay_3'].apply(len)\n",
    "df[\"essay4_len\"] = df['project_essay_4'].apply(len)\n",
    "df[\"title_len\"] = df['project_title'].apply(len)\n",
    "\n",
    "df['text'] = df.apply(lambda row: ' '.join([str(row['project_essay_1']), \n",
    "                                            str(row['project_essay_2']), \n",
    "                                            str(row['project_essay_3']), \n",
    "                                            str(row['project_essay_4'])]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
    "df['char_count'] = df['text'].apply(len)\n",
    "df['word_density'] = df['char_count'] / (df['word_count']+1)\n",
    "df['stopword_count'] = df['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.lower() in stop_words]))\n",
    "df['punctuation_count'] = df['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in punctuation))) \n",
    "df['upper_case_word_count'] = df['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))\n",
    "df['title_word_count'] = df['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['project_essay_1'] = df['project_essay_1'].apply(cleanup_text)\n",
    "df['project_essay_2'] = df['project_essay_2'].apply(cleanup_text)\n",
    "df['project_essay_3'] = df['project_essay_3'].apply(cleanup_text)\n",
    "df['project_essay_4'] = df['project_essay_4'].apply(cleanup_text)\n",
    "df['project_title'] = df['project_title'].apply(cleanup_text)\n",
    "df['description'] = df['description'].apply(cleanup_text)\n",
    "df['project_resource_summary'] = df['project_resource_summary'].apply(cleanup_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Year\"] = df[\"project_submitted_datetime\"].dt.year\n",
    "df[\"Month\"] = df[\"project_submitted_datetime\"].dt.month\n",
    "df['Weekday'] = df['project_submitted_datetime'].dt.weekday\n",
    "df[\"Hour\"] = df[\"project_submitted_datetime\"].dt.hour\n",
    "df[\"Month_Day\"] = df['project_submitted_datetime'].dt.day\n",
    "df[\"Year_Day\"] = df['project_submitted_datetime'].dt.dayofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['noun_count'] = df['temp_text'].apply(lambda x: pos_check(x, 'noun'))\n",
    "df['verb_count'] = df['temp_text'].apply(lambda x: pos_check(x, 'verb'))\n",
    "df['adj_count'] = df['temp_text'].apply(lambda x: pos_check(x, 'adj'))\n",
    "df['adv_count'] = df['temp_text'].apply(lambda x: pos_check(x, 'adv'))\n",
    "df['pron_count'] = df['temp_text'].apply(lambda x: pos_check(x, 'pron'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sent_polarity'] = df['temp_text'].apply(lambda x: get_polarity(x))\n",
    "df['sent_subjectivity'] = df['temp_text'].apply(lambda x: get_subjectivity(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 11,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "df['article_text'] = df.apply(lambda row: ' '.join([str(row['project_title']), \n",
    "                                            str(row['project_essay_1']), \n",
    "                                            str(row['project_essay_2']), \n",
    "                                            str(row['project_essay_3']), \n",
    "                                            str(row['project_essay_4'])]), axis=1)\n",
    "\n",
    "\n",
    "df['resource_text'] = df.apply(lambda row: ' '.join([str(row['description']), \n",
    "                                            str(row['project_resource_summary'])]), axis=1)\n",
    "\n",
    "\n",
    "complete_text = df['text']\n",
    "title_text = df['project_title']\n",
    "resource_text = df['resource_text']\n",
    "\n",
    "vect_word = TfidfVectorizer(max_features=8000, analyzer='word', stop_words='english', ngram_range=(1,3), dtype=np.float32) \n",
    "vect_word.fit(complete_text[traindex])\n",
    "tfidf_complete = vect_word.transform(complete_text[traindex])\n",
    "\n",
    "vect_word = TfidfVectorizer(max_features=3000, analyzer='word', stop_words='english', ngram_range=(1,3), dtype=np.float32) \n",
    "vect_word.fit(title_text[traindex])\n",
    "tfidf_title = vect_word.transform(title_text[traindex])\n",
    "\n",
    "vect_word = TfidfVectorizer(max_features=4000, analyzer='word', stop_words='english', ngram_range=(1,3), dtype=np.float32) \n",
    "vect_word.fit(resource_text[traindex])\n",
    "tfidf_resource = vect_word.transform(resource_text[traindex])\n",
    "\n",
    "\n",
    "char_word = TfidfVectorizer(max_features=8000, analyzer='char', stop_words='english', ngram_range=(1,3), dtype=np.float32) \n",
    "char_word.fit(complete_text[traindex])\n",
    "tfidf_complete = char_word.transform(complete_text[traindex])\n",
    "\n",
    "char_word = TfidfVectorizer(max_features=3000, analyzer='char', stop_words='english', ngram_range=(1,3), dtype=np.float32) \n",
    "char_word.fit(title_text[traindex])\n",
    "tfidf_title = char_word.transform(title_text[traindex])\n",
    "\n",
    "char_word = TfidfVectorizer(max_features=4000, analyzer='char', stop_words='english', ngram_range=(1,3), dtype=np.float32) \n",
    "char_word.fit(resource_text[traindex])\n",
    "tfidf_resource = char_word.transform(resource_text[traindex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = train.project_title.values\n",
    "xtest = test.project_title.values\n",
    "\n",
    "print (\"load the fast text vectors in a dictionary\")\n",
    "embeddings_index = {}\n",
    "count = 0\n",
    "\n",
    "f = open('data/wiki-news-300d-1M.vec', encoding=\"utf8\")\n",
    "for line in tqdm(f):\n",
    "    count += 1 \n",
    "    if count == 10:\n",
    "      break\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "print (\"keras preprocessing\")\n",
    "token = text.Tokenizer(num_words=100000)\n",
    "max_len = 300\n",
    "\n",
    "token.fit_on_texts(list(xtrain))\n",
    "xtrain_seq = token.texts_to_sequences(xtrain)\n",
    "xtest_seq = token.texts_to_sequences(xtest)\n",
    "\n",
    "print (\"zero pad the sequences\")\n",
    "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xtest_pad = sequence.pad_sequences(xtest_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index\n",
    "\n",
    "print (\"create an embedding matrix for the words we have in the dataset\")\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvectorizer = CountVectorizer(min_df=4,\n",
    "                              max_features=180000,\n",
    "                              tokenizer=tokenize,\n",
    "                              ngram_range=(1,2))\n",
    "cvz = cvectorizer.fit_transform(combined_sample['item_description'])\n",
    "lda_model = LatentDirichletAllocation(n_components=20,\n",
    "                                      learning_method='online',\n",
    "                                      max_iter=20,\n",
    "                                      random_state=42)\n",
    "X_topics = lda_model.fit_transform(cvz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words = 10\n",
    "topic_summaries = []\n",
    "\n",
    "topic_word = lda_model.components_  # get the topic words\n",
    "vocab = cvectorizer.get_feature_names()\n",
    "\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    topic_summaries.append(' '.join(topic_words))\n",
    "    print('Topic {}: {}'.format(i, ' | '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 18,
        "hidden": false,
        "row": 0,
        "width": null
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# target variable - is_approved\n",
    "\n",
    "is_approved = df[\"project_is_approved\"].value_counts()\n",
    "labels = (np.array(is_approved.index))\n",
    "sizes = (np.array((is_approved / approval_dist.sum())*100))\n",
    "\n",
    "trace = go.Bar(x=labels, y=sizes, marker=dict(color=['green', 'red']))\n",
    "layout = go.Layout(title = \"Project Approval Distribution\", width = 600, height=500)\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 31,
        "hidden": false,
        "row": 18,
        "width": null
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# target variable - times\n",
    "\n",
    "def get_label_sizes(colname):\n",
    "    dist = df[colname].value_counts()\n",
    "    labels = (np.array([str(_) for _ in list(dist.index)]))\n",
    "    sizes = (np.array((dist / dist.sum())*100))\n",
    "    trace = go.Bar(x=labels, y=sizes)\n",
    "    return trace\n",
    "trace1 = get_label_sizes('Year')\n",
    "trace2 = get_label_sizes('Month')\n",
    "trace3 = get_label_sizes('Weekday')\n",
    "trace4 = get_label_sizes('Hour')\n",
    "trace5 = get_label_sizes('Month_Day')\n",
    "trace6 = get_label_sizes('Year_Day')\n",
    "\n",
    "\n",
    "fig = tools.make_subplots(rows=3, cols=2)\n",
    "fig.append_trace(trace1, 1, 1)\n",
    "fig.append_trace(trace2, 1, 2)\n",
    "fig.append_trace(trace3, 2, 1)\n",
    "fig.append_trace(trace4, 2, 2)\n",
    "fig.append_trace(trace5, 3, 1)\n",
    "fig.append_trace(trace6, 3, 2)\n",
    "\n",
    "fig['layout'].update(height=800, width=1000, title='')\n",
    "py.iplot(fig, filename='simple-subplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 31,
        "hidden": false,
        "row": 49,
        "width": null
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# target variable - teachers\n",
    "\n",
    "def get_label_sizes(colname):\n",
    "    dist = df[colname].value_counts()\n",
    "    labels = (np.array([str(_) for _ in list(dist.index)]))\n",
    "    sizes = (np.array((dist / dist.sum())*100))\n",
    "    trace = go.Bar(x=labels, y=sizes)\n",
    "    return trace\n",
    "trace1 = get_label_sizes('teacher_prefix')\n",
    "trace2 = get_label_sizes('school_state')\n",
    "# trace3 = get_label_sizes('teacher_prefix')\n",
    "trace4 = get_label_sizes('project_grade_category')\n",
    "trace5 = get_label_sizes('project_subject_categories')\n",
    "trace6 = get_label_sizes('project_subject_subcategories')\n",
    "\n",
    "fig = tools.make_subplots(rows=3, cols=2)\n",
    "fig.append_trace(trace1, 1, 1)\n",
    "fig.append_trace(trace2, 1, 2)\n",
    "# fig.append_trace(trace3, 2, 1)\n",
    "fig.append_trace(trace4, 2, 2)\n",
    "fig.append_trace(trace5, 3, 1)\n",
    "fig.append_trace(trace6, 3, 2)\n",
    "\n",
    "fig['layout'].update(height=800, width=1000, title='')\n",
    "py.iplot(fig, filename='simple-subplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 31,
        "hidden": false,
        "row": 80,
        "width": null
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "trace1 = go.Histogram(x=df[\"price\"], nbinsx = 50, opacity=0.75)\n",
    "trace2 = go.Histogram(x=np.log(df[\"price\"]), nbinsx = 50, opacity=0.75)\n",
    "trace3 = go.Histogram(x=df[\"quantity\"], nbinsx = 50, opacity=0.75)\n",
    "trace4 = go.Histogram(x=np.log(df[\"quantity\"]), nbinsx = 50, opacity=0.75)\n",
    "\n",
    "fig = tools.make_subplots(rows=2, cols=2)\n",
    "fig.append_trace(trace1, 1, 1)\n",
    "fig.append_trace(trace2, 1, 2)\n",
    "fig.append_trace(trace3, 2, 1)\n",
    "fig.append_trace(trace4, 2, 2)\n",
    "\n",
    "fig['layout'].update(height=800, width=1000, title='')\n",
    "py.iplot(fig, filename='simple-subplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 9,
        "hidden": false,
        "row": 4,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def generate_wordcloud(tup):\n",
    "    wordcloud = WordCloud(background_color='white', max_words=50, max_font_size=40, \n",
    "                        random_state=42).generate(str(tup))\n",
    "    return wordcloud\n",
    "\n",
    "\n",
    "article1 = Counter(df['project_essay_1']).most_common(100)\n",
    "article2 = Counter(df['project_essay_2']).most_common(100)\n",
    "article3 = Counter(df['project_essay_3']).most_common(100)\n",
    "article4 = Counter(df['project_essay_4']).most_common(100)\n",
    "article5 = Counter(df['project_title']).most_common(100)\n",
    "article6 = Counter(df['description']).most_common(100)\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(30, 15))\n",
    "\n",
    "ax = axes[0, 0]\n",
    "ax.imshow(generate_wordcloud(article1), interpolation=\"bilinear\")\n",
    "ax.axis('off')\n",
    "ax.set_title(\"article1\", fontsize=30)\n",
    "\n",
    "ax = axes[0, 1]\n",
    "ax.imshow(generate_wordcloud(article2))\n",
    "ax.axis('off')\n",
    "ax.set_title(\"article2\", fontsize=30)\n",
    "\n",
    "ax = axes[1, 0]\n",
    "ax.imshow(generate_wordcloud(article3))\n",
    "ax.axis('off')\n",
    "ax.set_title(\"article3\", fontsize=30)\n",
    "\n",
    "ax = axes[1, 1]\n",
    "ax.imshow(generate_wordcloud(article4))\n",
    "ax.axis('off')\n",
    "ax.set_title(\"article4\", fontsize=30)\n",
    "\n",
    "ax = axes[2, 0]\n",
    "ax.imshow(generate_wordcloud(article5))\n",
    "ax.axis('off')\n",
    "ax.set_title(\"title\", fontsize=30)\n",
    "\n",
    "ax = axes[2, 1]\n",
    "ax.imshow(generate_wordcloud(article6))\n",
    "ax.axis('off')\n",
    "ax.set_title(\"description\", fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 19,
        "hidden": false,
        "row": 111,
        "width": null
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "## bivariate analysis \n",
    "\n",
    "\n",
    "### Stacked Bar Chart ###\n",
    "def create_stack_bar_data(col):\n",
    "    x_values = df[col].value_counts().index.tolist()\n",
    "    y0_values = []\n",
    "    y1_values = []\n",
    "    for val in x_values:\n",
    "        y1_values.append(np.sum(df[\"project_is_approved\"][df[col]==val] == 1))\n",
    "        y0_values.append(np.sum(df[\"project_is_approved\"][df[col]==val] == 0))\n",
    "    trace1 = go.Bar(x = x_values, y = y1_values, name='Accepted Proposals')\n",
    "    trace2 = go.Bar(x = x_values, y = y0_values, name='Rejected Proposals')\n",
    "    layout = go.Layout(title = \"Project Grade Distribution\", barmode='group', width = 1000)\n",
    "    data = [trace1, trace2]\n",
    "    return data, layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "data1, layout1 = create_stack_bar_data('Year')\n",
    "fig = go.Figure(data=data1, layout=layout1)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2, layout2 = create_stack_bar_data('Month')\n",
    "fig = go.Figure(data=data2, layout=layout2)\n",
    "py.iplot(fig, filename='ProjectGradeCategory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3, layout3 = create_stack_bar_data('Weekday')\n",
    "fig = go.Figure(data=data3, layout=layout3)\n",
    "py.iplot(fig, filename='ProjectGradeCategory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data4, layout4 = create_stack_bar_data('Hour')\n",
    "fig = go.Figure(data=data4, layout=layout4)\n",
    "py.iplot(fig, filename='ProjectGradeCategory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data5, layout5 = create_stack_bar_data('Month_Day')\n",
    "fig = go.Figure(data=data5, layout=layout5)\n",
    "py.iplot(fig, filename='ProjectGradeCategory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data6, layout6 = create_stack_bar_data('Year_Day')\n",
    "fig = go.Figure(data=data6, layout=layout6)\n",
    "py.iplot(fig, filename='ProjectGradeCategory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data6, layout6 = create_stack_bar_data('teacher_prefix')\n",
    "fig = go.Figure(data=data6, layout=layout6)\n",
    "py.iplot(fig, filename='ProjectGradeCategory')\n",
    "\n",
    "data6, layout6 = create_stack_bar_data('school_state')\n",
    "fig = go.Figure(data=data6, layout=layout6)\n",
    "py.iplot(fig, filename='ProjectGradeCategory')\n",
    "\n",
    "data6, layout6 = create_stack_bar_data('project_grade_category')\n",
    "fig = go.Figure(data=data6, layout=layout6)\n",
    "py.iplot(fig, filename='ProjectGradeCategory')\n",
    "\n",
    "data6, layout6 = create_stack_bar_data('project_subject_categories')\n",
    "fig = go.Figure(data=data6, layout=layout6)\n",
    "py.iplot(fig, filename='ProjectGradeCategory')\n",
    "\n",
    "data6, layout6 = create_stack_bar_data('project_subject_subcategories')\n",
    "fig = go.Figure(data=data6, layout=layout6)\n",
    "py.iplot(fig, filename='ProjectGradeCategory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "grid_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
